---
title: Setting up SLOs with Express
date: 2023-07-04
---

import { Callout } from "nextra-theme-docs"

# Setting up SLOs with an Express

In this guide we'll go over what are Service Level Objectives (SLO), why you would want to adopt them for your alerting, and how you can accomplish that in your Express application.

## Why SLOs

Chances are - your uptime alerts aren't great.

They may be either firing too often, bothering you enough to mute that Slack channel, or they are too slow at notifying when they should have and you end up getting calls from your customer support.

If they're based on logs, they are likely relying on information that is delayed (e.g.: while your logs are ingesting) and may be unreliable (due to the changing nature of logs).

Finally, they are also probably hard to use to assess how affected your users actually are by the error. If my `service_proxy_02` is acting up - what does that *actually* mean for people using the website?

A better way to set up alerts is using the Service Level Objectives or SLOs. Popularized in the [Google SRE book](https://sre.google/workbook/implementing-slos/) - SLOs enable engineers to set up alerts for user-focused teams. SLO alerts align best with the user experience, e.g. response and error rates of a service, are actionable and can often give you a "gas leak" warning before the issue escalates.

So how do they work?

## How SLOs work

Here comes the ~science~ math bit:

SO how do they actually work? Well, there’s two key concepts that need to be understood in order to explain this, **error budgets** and **burn rates**.

An error budget is the amount of errors that your SLO allows for during normal operations. For example if you have an error SLO of 99% over a 28 day period, then what you’re saying is that the service is allowed to throw 1% of errors over that period of time. For every 100 requests the service receives, one request can generate an error and the service is still working exactly to its SLO. This number of allowed errors is your error budget, they’re budgeted into the SLO. Put in mathematical terms the error budget can be described as:

$\text{Error budget} = 1 - SLO$

So for an SLO of 99% the error budget is:

$1 - 0.99 = 0.01$

In practical terms this means that the error budget is one request in every hundred.

The burn rate takes this a bit further. The burn rate is the rate at which a service is burning through its error budget, where a burn rate of 1 would perfectly consume the error budget within the time window. For our previous example with an SLO of 99.9% over a 28 day period a burn rate of 1 would be consistently having one request generate an error in every thousand requests for that 28 day period. If the burn rate increases above this then the service is burning its error budget too fast and will likely consume it before the 28 days is up. For example, a burn rate of 2 would mean that the error budget is consumed within just 14 days!

The burn rate makes up the basis of the SLO-based alert.

One final note before we dive in: SLOs are part-science and part-art. Hopefully this guide gives you a confident place to start, but you will likely need to tweak the values that fit your precise business needs and processes. The best way to learn SLOs after all, like any concept in engineering, is to go build with them.

### How SLOs (and alerting) work in Autometrics

In the languages that we have Autometrics library for, you can create your SLOs directly in the same code as your application.

...
    

## Implementing SLOs in Express (using Autometrics)

Ok let's get to the real part of this guide.

<Callout type="info">

#### Pre-requisites

- **Autometrics library for TypeScript/JavaScript** installed: see [quickstart](/typescript/quickstart) for installation instructions.
- **Prometheus**. A powerful open source monitoring and alerting database that Autometrics is designed to interface with. You can get started with it locally quickly using the [Autometrics CLI](/local-development) or download the binary directly.

</Callout>

Let's take a toy example Express API that models a basic user directory. Currently it has a single endpoint /users where accepts requests to manage the user directory (create, update, delete users).

Our API code is split up into two modules (files):

-   server.ts - our main entry point, we'll focus on this file mostly for the guide;

```typescript filename="server.ts"
import express, { Request, Response } from "express";
import { 
  handleCreateUser,
  handleDeleteUser,
  handleUpdateUser,
  handleGetAllUsers,
  handleGetUserById
} from "./routes.js"
  
const app = express();

app.get("/users", handleGetAllUsers);
app.get("/users/:id", handleGetUserById);
app.post("/users", handleCreateUser);
app.put("/users/:id", handleUpdateUser);
app.delete("/users/:id", handleDeleteUser);

app.listen(3000, () => {
  console.log("Server started on port 3000");
});

```

-   routes.ts  - where all of our handlers are defined.
    
```typescript filename="routes.ts"
import { Request, Response } from "express";

export function handleGetAllUsers(req: Request, res: Response) {
  // ...
}

export function handleGetUserById(req: Request, res: Response) {
 // ...
}

export function handleCreateUser(req: Request, res: Response) {
  // ...
}

export function handleUpdateUser(req: Request, res: Response) {
  // ...
}

export function handleDeleteUser(req: Request, res: Response) {
  // ...
}
```
  

### Adding Autometrics to the project

The first thing we need to do is to make sure that we're generating the metrics necessary to create our SLOs. We can do that simply by wrapping our handlers in a `autometrics` higher order function.
  
```typescript filename="server.ts" {1,8-12}
import { autometrics } from "@autometrics/autometrics"
import express, { Request, Response } from "express";

//...

const app = express();

app.get("/users", autometrics(handleGetAllUsers));
app.get("/users/:id", autometrics(handleGetUserById));
app.post("/users", autometrics(handleCreateUser));
app.put("/users/:id", autometrics(handleUpdateUser));
app.delete("/users/:id", autometrics(handleDeleteUser));


```

You can use the same `autometrics` interface to continue wrapping other functions that are called inside your handlers, for example any functions that are calling out to the database or doing any other heavy business logic. 

It's quite simple: the more you add - the better a picture you will be able to build from your metrics data.

For each wrapped function, autometrics will create metrics that are necessary to assess basic availability questions like:
- Rate: how many requests has this function received?
- Error: how many of them errored?
- Duration: how long do the requests take?

These are the basic building blocks we'll use to build our Service Level Objectives.

### Creating an SLO

We can create our SLO right in the same place - the `@autometrics/autometrics` library includes helper utilities for that: `Objective`, `ObjectiveLatency`, `ObjectivePercentile`.

```typescript filename="server.ts"
import {
  autometrics,
  Objective,
  ObjectiveLatency,
  ObjectivePercentile,
} from "@autometrics/autometrics";
import express, { Request, Response } from "express";

// ...

```
  
All of these utilities are simply interfaces that you can use to create an object that we can then pass to each of our `autometrics` wrapped handlers. 

Here's how that works.

```typescript
const API_SLO: Objective = {
    name: "api",
    successRate: ObjectivePercentile.P99,
    latency: [ObjectiveLatency.Ms250, ObjectivePercentile.P99]
}
```

First, we create an `Objective` object that we're gonna call `API_SLO`. In this object we need to pass a descriptive `name`, and two parameters: `successRate` and `latency`. 

The `successRate` accepts the `ObjectivePercentile` enum value - for example `ObjectivePercentile.P99` for an objective of success rate of 99%. 

The `latency` accepts a tuple `[A, B]` which should include the latency goal in milliseconds and similarly a percentile. So: `[ObjectiveLatency.Ms250, ObjectivePercentile.P99]` translates that we _expect 99% of requests to be completed within 250 milliseconds_.


We can now pass in our `API_SLO` into each of our wrapped handlers:

```typescript filename="server.ts" {14-18}
import { autometrics } from "@autometrics/autometrics"
import express, { Request, Response } from "express";

//...

const app = express();

const API_SLO: Objective = {
    name: "api",
    successRate: ObjectivePercentile.P99,
    latency: [ObjectiveLatency.Ms250, ObjectivePercentile.P99]
}

app.get("/users", autometrics({ objective: API_SLO }, handleGetAllUsers));
app.get("/users/:id", autometrics({ objective: API_SLO }, handleGetUserById));
app.post("/users", autometrics({ objective: API_SLO }, handleCreateUser));
app.put("/users/:id", autometrics({ objective: API_SLO }, handleUpdateUser));
app.delete("/users/:id", autometrics({ objective: API_SLO }, handleDeleteUser));
```
